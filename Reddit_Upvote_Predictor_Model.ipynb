{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import pickle\n",
    "from textblob import Word\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from collections import Counter\n",
    "import math\n",
    "%matplotlib inline \n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#size of reddit posts in terms of no of posts to fetch from database\n",
    "no_of_posts = 100000\n",
    "chunk_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Setting column names for training data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reading data from sql database reddit_data.db\n",
    "con = sqlite3.connect(\"reddit_data.db\")\n",
    "sql_query = \"select title, subreddit, domain from reddit_posts where random_number != 36 or random_number != 53 or random_number != 24 limit 100000\"\n",
    "df = pd.read_sql(sql_query, con)\n",
    "con.close()\n",
    "\n",
    "\n",
    "unique_values_subreddit = df[\"subreddit\"].unique().tolist()\n",
    "unique_values_subreddit = [\"subreddit_\" + str(s) for s in unique_values_subreddit]\n",
    "unique_values_domain = df[\"domain\"].unique().tolist()\n",
    "unique_values_domain = [\"domain_\" + s for s in unique_values_domain]\n",
    "\n",
    "df[\"title\"]=df[\"title\"].map(lambda x: x.lower())\n",
    "df[\"title\"] = df[\"title\"].map(lambda x: re.sub('[^a-zA-Z ]', '', x))\n",
    "df[\"title\"] = df[\"title\"].map(lambda x: x.split())\n",
    "df[\"title\"] = df[\"title\"].map(lambda x: [i for i in x if (len(i)>3 and len(i)<10)])\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "df[\"title\"] = df[\"title\"].map(lambda x: [i for i in x if i not in stop])\n",
    "df[\"title\"] = df[\"title\"].map(lambda x: [str(i) for i in x])\n",
    "\n",
    "lists = df[\"title\"].tolist()\n",
    "unique_values_title = list(set([item for sublist in lists for item in sublist]))\n",
    "training_df_column_names = unique_values_domain + unique_values_subreddit + unique_values_title\n",
    "\n",
    "#saving the list of training_df_column_names in a pkl file:\n",
    "with open(\"training_df_column_names.pkl\",\"wb\") as f:\n",
    "    pickle.dump(training_df_column_names, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tf- idf Values for words in title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tf_idf dictionary\n",
    "tfidf_dict ={}\n",
    "for w in unique_values_title:\n",
    "    tfidf_dict[w] = len([True for sublist in lists if w in sublist])\n",
    "\n",
    "for key in tfidf_dict:\n",
    "    tfidf_dict[key] = math.log( (no_of_posts / (1+tfidf_dict[key]) ))\n",
    "\n",
    "with open(\"tfidf_dict.pkl\",'wb') as f:\n",
    "    pickle.dump(tfidf_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing data for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def df_format(df):\n",
    "    if \"title\" in df.columns:\n",
    "        df[\"title\"]=  df[\"title\"].map(lambda x: x.lower())\n",
    "        df[\"title\"] = df[\"title\"].map(lambda x: re.sub('[^a-zA-Z ]', '', x))\n",
    "        df[\"title\"] = df[\"title\"].map(lambda x: x.split())\n",
    "        df[\"title\"] = df[\"title\"].map(lambda x: [i for i in x if (len(i)>3 and len(i)<10)])\n",
    "        from nltk.corpus import stopwords\n",
    "        stop = stopwords.words('english')\n",
    "        df[\"title\"] = df[\"title\"].map(lambda x: [i for i in x if i not in stop])\n",
    "        df[\"title\"] = df[\"title\"].map(lambda x: [str(i) for i in x])\n",
    "    if \"domain\" in df.columns:\n",
    "        df[\"domain\"] = df[\"domain\"].map(lambda x: \"domain_\"+str(x))\n",
    "    if \"subreddit\" in df.columns:\n",
    "        df[\"subreddit\"] = df[\"subreddit\"].map(lambda x: \"subreddit_\"+str(x))\n",
    "    return df\n",
    "\n",
    "#loading the training_df_column_names.pkl\n",
    "with open(\"training_df_column_names.pkl\",\"rb\") as f:\n",
    "    training_df_column_names = pickle.load(f)\n",
    "\n",
    "#loading tf-idf values for all words in title\n",
    "with open(\"tfidf_dict.pkl\",\"rb\") as f:\n",
    "    tfidf_dict = pickle.load(f)\n",
    "\n",
    "#Reading the training values from database\n",
    "con = sqlite3.connect(\"reddit_data.db\")\n",
    "sql_query = \"select title, subreddit, domain from reddit_posts where random_number != 36 or random_number != 53 or random_number != 24 limit 100000\"\n",
    "\n",
    "df = pd.read_sql(sql_query, con, chunksize=100)\n",
    "print('sql')\n",
    "for i,d in enumerate(df):\n",
    "    training_df = pd.DataFrame(0, index=np.arange(int(d.shape[0])), columns = training_df_column_names)\n",
    "    d = df_format(d)\n",
    "    for index, row in d.iterrows():\n",
    "        \n",
    "        if str(row[\"domain\"]) in training_df_column_names:\n",
    "            training_df.loc[index, row[\"domain\"]] += 1\n",
    "        if str(row[\"subreddit\"]) in training_df_column_names:\n",
    "            training_df.loc[index, row[\"subreddit\"]] += 1\n",
    "        for w in row[\"title\"]:\n",
    "            if w in training_df_column_names:\n",
    "                training_df.loc[index,(w)] += 1\n",
    "    \n",
    "    for w in list(set([item for sublist in d[\"title\"].tolist()  for item in sublist])):\n",
    "        training_df[w] = training_df[w]*tfidf_dict[w]\n",
    "    name_training = 'msg_data/training_df_'+str(i)+'.msg'\n",
    "    name_ups = 'msg_data/ups_df_'+str(i)+'.msg'\n",
    "    d[\"ups\"].to_msgpack(name_ups, use_bin_type=True)\n",
    "    training_df.to_msgpack(name_training, use_bin_type=True)\n",
    "    gc.collect()\n",
    "    del training_df\n",
    "con.close()\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "model = SGDRegressor(alpha=0.000050)\n",
    "\n",
    "for i in range(no_of_posts/chunk_size):\n",
    "    \n",
    "    x = pd.read_msgpack('msg_data/training_df_'+str(i)+'.msg')\n",
    "        \n",
    "    y = pd.read_msgpack('msg_data/ups_df_'+str(i)+'.msg')\n",
    "    y=y.apply(lambda x: math.log(x) if x!=0 else x ).as_matrix()\n",
    "    model.partial_fit(x,y)\n",
    "    \n",
    "#model_list.append(model)\n",
    "filename = 'model_trained.pkl'\n",
    "with open(filename, \"wb\") as f:\n",
    "    pickle.dump(model, f)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
